{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bikasherl/miniconda3/envs/bajra/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST,EMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    return (torch.exp(Z)/torch.sum(torch.exp(Z),dim=1,keepdim=True))\n",
    "\n",
    "def categorical_cross_entropy(Y_enc_train,Y_hat):\n",
    "    return torch.sum((-Y_enc_train*torch.log(Y_hat)),dim=1,keepdim=True).mean()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset EMNIST\n",
       "     Number of datapoints: 112800\n",
       "     Root location: emnist_dataset/train/\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset EMNIST\n",
       "     Number of datapoints: 18800\n",
       "     Root location: emnist_dataset/test/\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edata=EMNIST(root='emnist_dataset/train/',split='balanced',train=True,download=True,transform=ToTensor())\n",
    "test_edata=EMNIST(root='emnist_dataset/test/',split='balanced',train=False,download=True,transform=ToTensor())\n",
    "train_edata,test_edata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([112800, 784]),\n",
       " torch.Size([112800, 1]),\n",
       " torch.Size([18800, 784]),\n",
       " torch.Size([18800, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train set\n",
    "X_etrain=torch.flatten(train_edata.data,1,2).to(torch.float32)\n",
    "Y_etrain=train_edata.targets.reshape(X_etrain.shape[0],1).to(torch.int8)\n",
    "\n",
    "X_etrain.shape,Y_etrain.shape\n",
    "\n",
    "# test set\n",
    "X_etest=torch.flatten(test_edata.data,1,2).to(torch.float32)\n",
    "Y_etest=test_edata.targets.reshape(X_etest.shape[0],1).to(torch.int8)\n",
    "\n",
    "X_etrain.shape,Y_etrain.shape,X_etest.shape,Y_etest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling train dataset\n",
    "for i in range(X_etrain.shape[0]):\n",
    "    X_etrain[i]=(X_etrain[i]-X_etrain[i].min())/(X_etrain[i].max()-X_etrain[i].min())\n",
    "\n",
    "\n",
    "# scaling test dataset\n",
    "for i in range(X_etest.shape[0]):\n",
    "    X_etest[i]=(X_etest[i]-X_etest[i].min())/(X_etest[i].max()-X_etest[i].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([112800, 47]), torch.Size([18800, 47]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## One hot encoding For the labels\n",
    "\n",
    "Y_enc_etrain=torch.zeros((Y_etrain.shape[0],len(train_edata.classes))).to(torch.int8)\n",
    "\n",
    "for i in range(Y_etrain.shape[0]):\n",
    "    Y_enc_etrain[i][Y_etrain[i].item()]=1\n",
    "\n",
    "\n",
    "Y_enc_etest=torch.zeros((Y_etest.shape[0],len(test_edata.classes))).to(torch.int8)\n",
    "\n",
    "for i in range(Y_etest.shape[0]):\n",
    "    Y_enc_etest[i][Y_etest[i].item()]=1\n",
    "\n",
    "\n",
    "Y_enc_etrain.shape,Y_enc_etest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape- W1: torch.Size([784, 128])       \n",
      "shape- b1: torch.Size([1, 128])      \n",
      "shape- W2: torch.Size([128, 47])      \n",
      "shape- b2: torch.Size([1, 47])\n"
     ]
    }
   ],
   "source": [
    "num_input=X_etrain.shape[1]\n",
    "num_hidden=128\n",
    "\n",
    "\n",
    "# kaiming weight initialization\n",
    "W1=torch.randn((num_input,num_hidden),dtype=torch.float32,requires_grad=True)*torch.sqrt(torch.tensor(2/num_input))\n",
    "b1=torch.randn((1,num_hidden),dtype=torch.float32,requires_grad=True)*torch.sqrt(torch.tensor(2/num_input))\n",
    "\n",
    "W2=torch.randn((num_hidden,Y_enc_etrain.shape[1]),dtype=torch.float32,requires_grad=True)*torch.sqrt(torch.tensor(2/num_hidden))\n",
    "b2=torch.randn((1,Y_enc_etrain.shape[1]),dtype=torch.float32,requires_grad=True)*torch.sqrt(torch.tensor(2/num_hidden))\n",
    "\n",
    "\n",
    "print(f\"shape- W1: {W1.shape}\\\n",
    "       \\nshape- b1: {b1.shape}\\\n",
    "      \\nshape- W2: {W2.shape}\\\n",
    "      \\nshape- b2: {b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utitlity function to check manually calculated gradients with that of pytorch autograd\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 784]), torch.Size([32, 47]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size=32\n",
    "\n",
    "# a single batch\n",
    "X_btrain=X_etrain[:batch_size,:]\n",
    "Y_btrain=Y_enc_etrain[:batch_size,:]\n",
    "\n",
    "gamma=torch.randn((1,num_hidden),requires_grad=True)\n",
    "beta=torch.randn((1,num_hidden),requires_grad=True)\n",
    "\n",
    "X_btrain.shape,Y_btrain.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# forward pass\n",
    "Z1=X_btrain@W1 + b1\n",
    "A1=torch.relu(Z1)\n",
    "\n",
    "A1_mean=torch.sum(A1,dim=0,keepdim=True)/A1.shape[0]\n",
    "A1_std=(torch.sqrt(torch.sum(((A1-A1_mean)**2),dim=0,keepdim=True)/A1.shape[0]))+1e-10 # adding small term with std to eliminate dividing by 0 at A1_norm\n",
    "A1_norm=(A1-A1_mean+1e-10)/(A1_std)   \n",
    "\n",
    "# (1/(A1.shape[0]**(1/2)))*(0.5/((torch.sum(((A1-A1_mean)**2),dim=0)))**(1/2))*-2*(A1-A1_mean)\n",
    "\n",
    "# batch normalization\n",
    "A2=A1_norm*gamma+beta\n",
    "\n",
    "Z2=A2@W2 + b2\n",
    "Y_ehat=softmax(Z2)\n",
    "\n",
    "Loss=categorical_cross_entropy(Y_btrain,Y_ehat)\n",
    "# Loss=torch.sum((-Y_enc_etrain*torch.log(Y_ehat)),dim=1,keepdim=True).mean()\n",
    "\n",
    "loss_list_train.append(Loss.item())\n",
    "\n",
    "# # backpropgation\n",
    "dL_dZ2=(Y_ehat-Y_btrain)/X_btrain.shape[0]\n",
    "dZ2_dW2=A2\n",
    "\n",
    "dL_dW2=dZ2_dW2.T@dL_dZ2\n",
    "dL_db2=torch.sum(dL_dZ2,dim=0,keepdim=True)\n",
    "\n",
    "\n",
    "dZ2_dA2=W2\n",
    "dL_dA2= dL_dZ2 @ dZ2_dA2.T\n",
    "\n",
    "dA2_dgamma=A1_norm\n",
    "dL_dgamma=torch.sum(dL_dA2*dA2_dgamma,dim=0,keepdim=True)\n",
    "dL_dbeta=torch.sum(dL_dA2,dim=0,keepdim=True)\n",
    "\n",
    "dA2_dA1_norm=gamma  #---> shape (1,128)\n",
    "dL_dA1_norm=dL_dA2*dA2_dA1_norm    # ----> (32,128)\n",
    "dA1_norm_dA1_std= -(A1-A1_mean)*(A1_std**-2)\n",
    "dA1_norm_dA1_mean=-1/A1_std\n",
    "dL_dA1_std=torch.sum(dL_dA1_norm*dA1_norm_dA1_std,dim=0,keepdim=True)           \n",
    "dL_dA1_mean=torch.sum(dL_dA1_norm*dA1_norm_dA1_mean,dim=0,keepdim=True) \n",
    "\n",
    "dA1_std_dA1_mean=(1/(A1.shape[0]**(1/2)))*(0.5/((torch.sum(((A1-A1_mean)**2),dim=0,keepdim=True)))**(1/2))*-2*(A1-A1_mean)\n",
    "dL_dA1_mean+=torch.sum(dL_dA1_std*dA1_std_dA1_mean,dim=0,keepdim=True)\n",
    "\n",
    "# dA1_std_dA1=(1/(A1.shape[0]**(1/2)))*(0.5/((torch.sum(((A1-A1_mean+1e-16)**2),dim=0,keepdim=True)))**(1/2))*2*(A1-A1_mean+1e-16)\n",
    "# dL_dA1=dL_dA1_std*dA1_std_dA1\n",
    "\n",
    "# dA1_mean_dA1=torch.ones_like(A1)/A1.shape[0]\n",
    "# dL_dA1+=dL_dA1_mean*dA1_mean_dA1\n",
    "\n",
    "# dA1_dZ1=torch.where((Z1<0),0, 1)\n",
    "\n",
    "# dL_dZ1=(dA1_dZ1 * dL_dA1)\n",
    "\n",
    "# dZ1_dW1=X_etrain\n",
    "\n",
    "# dL_dW1=dZ1_dW1.T @ dL_dZ1\n",
    "# dL_db1=torch.sum(dL_dZ1,dim=0,keepdim=True)\n",
    "\n",
    "# # weights and biases optimization\n",
    "# W2-=0.9*dL_dW2\n",
    "# b2-=0.9*dL_db2\n",
    "# W1-=0.9*dL_dW1\n",
    "# b1-=0.9*dL_db1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [Z2,A2,A1_norm,gamma,beta,A1_std,A1_mean,W1,W2,b1,b2,A1,Z1]:\n",
    "    each.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1046e-01, -9.1469e-03, -8.4152e-02, -2.0692e-02,  4.9204e-01,\n",
       "         -8.8017e-02,  9.2528e-02,  1.1139e-01,  1.4220e-03, -9.3558e-02,\n",
       "         -1.6063e-01, -9.4405e-03,  3.3910e-01, -8.2059e-02,  9.0849e-05,\n",
       "          1.5636e+00, -4.5657e-02,  6.2388e-02, -9.3670e-02, -7.2527e-05,\n",
       "         -1.4325e-02, -1.4967e-02, -3.2541e-01,  2.3389e-02,  1.5310e-01,\n",
       "          4.3715e-01, -2.3531e-02, -6.8908e-02,  3.0556e-02, -4.1656e-02,\n",
       "          9.4008e-02, -1.1572e-01,  1.4871e-01, -2.1040e-01, -1.0840e-02,\n",
       "          3.3909e-01,  1.8660e-01, -1.0177e-01,  1.4038e-01, -8.2930e-03,\n",
       "          6.0822e-02, -3.6075e-02,  3.6233e-01, -7.3473e-04, -1.1976e-01,\n",
       "          3.1110e-01, -2.0340e-01,  8.4916e-03,         nan,  2.5956e-01,\n",
       "                 nan, -1.8628e-01, -1.5665e-02, -2.5812e-02, -1.4362e-01,\n",
       "         -6.5050e-02, -2.4336e-02,  4.1878e-02, -5.0967e-01,  1.2634e-02,\n",
       "          4.8483e-04, -8.2025e-03,  3.3736e+00,  4.5322e-02, -2.7738e-01,\n",
       "          2.6272e-01, -4.8051e-03, -1.5608e-01,  7.5808e-03, -4.0861e-02,\n",
       "         -2.8134e-03, -1.9343e-01,  1.3369e-03, -2.8119e-02, -2.5954e-01,\n",
       "          1.2410e-01, -5.4088e-03,  1.3446e-01, -1.4595e-02,  2.5791e-02,\n",
       "          9.0858e-01,  2.3713e+00, -2.9651e-02,  1.6478e-03,  5.9542e-01,\n",
       "         -3.1117e-01, -3.2559e-02, -1.1602e-01, -5.0290e-01, -2.8976e-01,\n",
       "          1.1045e-01, -7.2864e-01,  2.4682e-02, -1.0945e-01,  6.1085e-02,\n",
       "          8.5262e-02, -3.5477e+00, -1.0095e-01, -8.2213e-02, -2.2449e-02,\n",
       "          1.4176e-01, -3.1183e-01, -6.1597e-02, -5.0000e-03, -2.5807e-02,\n",
       "          4.1185e-01,  2.9597e-01, -4.1058e-01, -4.5537e-01, -1.8854e-01,\n",
       "         -8.1779e-02,  7.2941e-01,  2.9146e-01,  2.4600e-02,  1.3089e-04,\n",
       "          1.0538e-01,  8.2668e-02, -5.7705e-02, -1.5565e-01, -1.2845e-02,\n",
       "          1.9466e-01,  5.1676e-02,  1.9035e+00,  1.0488e-02,  6.3456e-03,\n",
       "          1.8002e-03,  3.3601e-02,  1.6228e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1_mean.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dZ2             | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dW2             | exact: False | approximate: True  | maxdiff: 5.960464477539063e-08\n",
      "db2             | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "dA2             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dgamma          | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "dbeta           | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
      "dA1_norm        | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "dA1_std         | exact: False | approximate: False | maxdiff: 334385888.0\n",
      "dA1_mean        | exact: False | approximate: False | maxdiff: nan\n"
     ]
    }
   ],
   "source": [
    "cmp(\"dZ2\", dL_dZ2, Z2)\n",
    "cmp(\"dW2\", dL_dW2, W2)\n",
    "cmp(\"db2\", dL_db2, b2)\n",
    "cmp(\"dA2\", dL_dA2, A2)\n",
    "cmp(\"dgamma\", dL_dgamma, gamma)\n",
    "cmp(\"dbeta\", dL_dbeta, beta)\n",
    "cmp(\"dA1_norm\", dL_dA1_norm, A1_norm)\n",
    "cmp(\"dA1_std\", dL_dA1_std, A1_std)\n",
    "cmp(\"dA1_mean\", dL_dA1_mean, A1_mean)\n",
    "# cmp(\"dA1\", dL_dA1, A1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bajra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
